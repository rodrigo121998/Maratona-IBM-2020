{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# MARATONA BEHIND THE CODE 2020\n\n## DESAFIO 2: PARTE 2"}, {"metadata": {}, "cell_type": "markdown", "source": "### Introducci\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "En la parte 1 de este desaf\u00edo, se realiz\u00f3 el procesamiento previo y el entrenamiento de un modelo a partir de un conjunto de datos base proporcionados. En este segundo paso, se integrar\u00e1 todas las transformaciones y eventos de entrenamiento creados previamente en un Pipeline completo para *deploy* en **Watson Machine Learning**."}, {"metadata": {}, "cell_type": "markdown", "source": "### Preparaci\u00f3n del Notebook"}, {"metadata": {}, "cell_type": "markdown", "source": "Primero realizaremos la instalaci\u00f3n do scikit-learn y la importaci\u00f3n de las mismas bibliotecas utilizadas anteriormente"}, {"metadata": {}, "cell_type": "code", "source": "!pip install scikit-learn==0.20.0 --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import json\nimport requests\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Es necesario volver a insertar el conjunto de datos base como un pandas dataframe, siguiendo las instrucciones\n\n![alt text](https://i.imgur.com/K1DwL9I.png \"importing-csv-as-df\")\n\nDespu\u00e9s de seleccionar la opci\u00f3n **\"Insert to code\"**, la celda de abajo se llenar\u00e1 con el c\u00f3digo necesario para importar y leer los datos en el archivo .csv como un Pandas DataFrame."}, {"metadata": {}, "cell_type": "code", "source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_06493f28c6f549ea8a61a6774686e746 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='nWcn4eXyU-wHqnFz0muFYVKZKh4GQwRHraHNPpOTOriT',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_06493f28c6f549ea8a61a6774686e746.get_object(Bucket='desafio2-donotdelete-pr-2ij22nwwu9ewnz',Key='dataset-tortuga-desafio-2.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_1 = pd.read_csv(body)\ndf_data_1.head()\n", "execution_count": 2, "outputs": [{"output_type": "execute_result", "execution_count": 2, "data": {"text/plain": "   Unnamed: 0               NAME   USER_ID  HOURS_DATASCIENCE  HOURS_BACKEND  \\\n0          28        Stormy Muto  58283940                7.0           39.0   \n1          81       Carlos Ferro   1357218               32.0            0.0   \n2          89  Robby Constantini  63212105               45.0            0.0   \n3         138       Paul Mckenny  23239851               36.0           19.0   \n4         143          Jean Webb  72234478               61.0           78.0   \n\n   HOURS_FRONTEND  NUM_COURSES_BEGINNER_DATASCIENCE  \\\n0            29.0                               2.0   \n1            44.0                               2.0   \n2            59.0                               0.0   \n3            28.0                               0.0   \n4            38.0                               6.0   \n\n   NUM_COURSES_BEGINNER_BACKEND  NUM_COURSES_BEGINNER_FRONTEND  \\\n0                           4.0                            0.0   \n1                           0.0                            0.0   \n2                           5.0                            4.0   \n3                           5.0                            7.0   \n4                          11.0                            0.0   \n\n   NUM_COURSES_ADVANCED_DATASCIENCE  NUM_COURSES_ADVANCED_BACKEND  \\\n0                               2.0                           5.0   \n1                               0.0                           5.0   \n2                               0.0                           4.0   \n3                               0.0                           5.0   \n4                               4.0                           3.0   \n\n   NUM_COURSES_ADVANCED_FRONTEND  AVG_SCORE_DATASCIENCE  AVG_SCORE_BACKEND  \\\n0                            0.0                   84.0               74.0   \n1                            0.0                   67.0               45.0   \n2                            1.0                    NaN               54.0   \n3                            3.0                    NaN               71.0   \n4                            0.0                   66.0               85.0   \n\n   AVG_SCORE_FRONTEND                PROFILE  \n0                 NaN     beginner_front_end  \n1                 NaN     beginner_front_end  \n2                47.0     advanced_front_end  \n3                89.0  beginner_data_science  \n4                 NaN     advanced_front_end  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>NAME</th>\n      <th>USER_ID</th>\n      <th>HOURS_DATASCIENCE</th>\n      <th>HOURS_BACKEND</th>\n      <th>HOURS_FRONTEND</th>\n      <th>NUM_COURSES_BEGINNER_DATASCIENCE</th>\n      <th>NUM_COURSES_BEGINNER_BACKEND</th>\n      <th>NUM_COURSES_BEGINNER_FRONTEND</th>\n      <th>NUM_COURSES_ADVANCED_DATASCIENCE</th>\n      <th>NUM_COURSES_ADVANCED_BACKEND</th>\n      <th>NUM_COURSES_ADVANCED_FRONTEND</th>\n      <th>AVG_SCORE_DATASCIENCE</th>\n      <th>AVG_SCORE_BACKEND</th>\n      <th>AVG_SCORE_FRONTEND</th>\n      <th>PROFILE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28</td>\n      <td>Stormy Muto</td>\n      <td>58283940</td>\n      <td>7.0</td>\n      <td>39.0</td>\n      <td>29.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>84.0</td>\n      <td>74.0</td>\n      <td>NaN</td>\n      <td>beginner_front_end</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>81</td>\n      <td>Carlos Ferro</td>\n      <td>1357218</td>\n      <td>32.0</td>\n      <td>0.0</td>\n      <td>44.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>67.0</td>\n      <td>45.0</td>\n      <td>NaN</td>\n      <td>beginner_front_end</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>89</td>\n      <td>Robby Constantini</td>\n      <td>63212105</td>\n      <td>45.0</td>\n      <td>0.0</td>\n      <td>59.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>54.0</td>\n      <td>47.0</td>\n      <td>advanced_front_end</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>138</td>\n      <td>Paul Mckenny</td>\n      <td>23239851</td>\n      <td>36.0</td>\n      <td>19.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>71.0</td>\n      <td>89.0</td>\n      <td>beginner_data_science</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>143</td>\n      <td>Jean Webb</td>\n      <td>72234478</td>\n      <td>61.0</td>\n      <td>78.0</td>\n      <td>38.0</td>\n      <td>6.0</td>\n      <td>11.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>85.0</td>\n      <td>NaN</td>\n      <td>advanced_front_end</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Construcci\u00f3n del Pipeline completo para el encapsulamiento en WML"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Preparando transformaciones personalizadas para cargar en WML"}, {"metadata": {}, "cell_type": "markdown", "source": "En el paso anterior, se mostr\u00f3 c\u00f3mo crear una transformaci\u00f3n personalizada, declarando una clase Python con los m\u00e9todos ``fit`` y ``transform``.\n\n    - C\u00f3digo de transformaci\u00f3n personalizada DropColumns():\n    \n    from sklearn.base import BaseEstimator, TransformerMixin\n    # All sklearn Transforms must have the `transform` and `fit` methods\n    class DropColumns(BaseEstimator, TransformerMixin):\n        def __init__(self, columns):\n            self.columns = columns\n        def fit(self, X, y=None):\n            return self\n        def transform(self, X):\n            # Primero copiamos el dataframe de entrada 'X' de entrada\n            data = X.copy()\n            # Devolvemos un nuevo marco de datos sin las columnas no deseadas\n            return data.drop(labels=self.columns, axis='columns')\n\nPara integrar estos tipos de transformaciones personalizadas con Pipelines en Watson Machine Learning, primero debe empaquetar su c\u00f3digo personalizado como una biblioteca de Python. Esto se puede hacer f\u00e1cilmente usando la herramienta *setuptools*.\n\nEn el siguiente repositorio de git: https://github.com/vnderlev/sklearn_transforms tenemos todos los archivos necesarios para crear un paquete de Python, llamado **my_custom_sklearn_transforms**.\nEste paquete tiene la siguiente estructura de archivos:\n\n    /my_custom_sklearn_transforms.egg-info\n        dependency_links.txt\n        not-zip-safe\n        PKG-INFO\n        SOURCES.txt\n        top_level.txt\n    /my_custom_sklearn_transforms\n        __init__.py\n        sklearn_transformers.py\n    PKG-INFO\n    README.md\n    setup.cfg\n    setup.py\n    \nEl archivo principal, que contendr\u00e1 el c\u00f3digo para nuestras transformaciones personalizadas, es el archivo **/my_custom_sklearn_transforms/sklearn_transformers.py**. Si accedes a \u00e9l en el repositorio, notar\u00e1s que contiene exactamente el mismo c\u00f3digo declarado en el primer paso (la clase DropColumns).\n\nSi has declarado sus propias transformaciones (adem\u00e1s de la DropColumn proporcionada), debes agregar todas las clases de esas transformaciones creadas en este mismo archivo. Para hacer esto, debes hacer fork de este repositorio (esto se puede hacer en la propia interfaz web de Github, haciendo clic en el bot\u00f3n como se muestra en la imagen a continuaci\u00f3n) y agregue sus clases personalizadas al archivo **sklearn_transformers.py**.\n\n![alt text](https://i.imgur.com/2lZ4Ty2.png \"forking-a-repo\")\n\nSi solo hizo uso de la transformaci\u00f3n proporcionada (DropColumns), puede omitir este paso de fork y continuar usando el paquete base provisto. :)\n\nDespu\u00e9s de preparar su paquete de Python con sus transformaciones personalizadas, reemplace el enlace del repositorio de git en la celda a continuaci\u00f3n y ejec\u00fatelo. Si no ha preparado ninguna transformaci\u00f3n nueva, ejecute la celda con el enlace del repositorio ya proporcionado.\n\n<hr>\n    \n**OBSERVACI\u00d3N**\n\nSi la ejecuci\u00f3n de la celda a continuaci\u00f3n devuelve un error de que el repositorio ya existe, ejecute:\n\n**!rm -r -f sklearn_transforms**"}, {"metadata": {}, "cell_type": "code", "source": "!rm -r -f sklearn_transforms", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# reemplace el enlace a continuaci\u00f3n con el enlace de su repositorio de git (si corresponde)\n!git clone https://github.com/rodrigo121998/sklearn_transforms.git", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Cloning into 'sklearn_transforms'...\nremote: Enumerating objects: 116, done.\u001b[K\nremote: Total 116 (delta 0), reused 0 (delta 0), pack-reused 116\u001b[K\nReceiving objects: 100% (116/116), 20.52 KiB | 0 bytes/s, done.\nResolving deltas: 100% (68/68), done.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "!cd sklearn_transforms\n!ls -ltr", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "total 52\r\n-rw-r----- 1 dsxuser dsxuser 45451 Aug 28 05:43 sklearn_transforms.zip\r\ndrwxr-x--- 5 dsxuser dsxuser  4096 Aug 28 05:57 sklearn_transforms\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Para cargar el c\u00f3digo en WML, necesitamos enviar un archivo .zip con todo el c\u00f3digo fuente, luego comprimiremos el directorio clonado a continuaci\u00f3n:"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "!zip -r sklearn_transforms.zip sklearn_transforms", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "updating: sklearn_transforms/ (stored 0%)\r\nupdating: sklearn_transforms/.git/ (stored 0%)\r\nupdating: sklearn_transforms/.git/packed-refs (deflated 10%)\r\nupdating: sklearn_transforms/.git/refs/ (stored 0%)\r\nupdating: sklearn_transforms/.git/refs/remotes/ (stored 0%)\r\nupdating: sklearn_transforms/.git/refs/remotes/origin/ (stored 0%)\r\nupdating: sklearn_transforms/.git/refs/remotes/origin/HEAD (stored 0%)\r\nupdating: sklearn_transforms/.git/refs/heads/ (stored 0%)\r\nupdating: sklearn_transforms/.git/refs/heads/master (stored 0%)\r\nupdating: sklearn_transforms/.git/refs/tags/ (stored 0%)\r\nupdating: sklearn_transforms/.git/logs/ (stored 0%)\r\nupdating: sklearn_transforms/.git/logs/refs/ (stored 0%)\r\nupdating: sklearn_transforms/.git/logs/refs/remotes/ (stored 0%)\r\nupdating: sklearn_transforms/.git/logs/refs/remotes/origin/ (stored 0%)\r\nupdating: sklearn_transforms/.git/logs/refs/remotes/origin/HEAD (deflated 29%)\r\nupdating: sklearn_transforms/.git/logs/refs/heads/ (stored 0%)\r\nupdating: sklearn_transforms/.git/logs/refs/heads/master (deflated 29%)\r\nupdating: sklearn_transforms/.git/logs/HEAD (deflated 29%)\r\nupdating: sklearn_transforms/.git/HEAD (stored 0%)\r\nupdating: sklearn_transforms/.git/hooks/ (stored 0%)\r\nupdating: sklearn_transforms/.git/hooks/pre-push.sample (deflated 50%)\r\nupdating: sklearn_transforms/.git/hooks/pre-commit.sample (deflated 46%)\r\nupdating: sklearn_transforms/.git/hooks/post-update.sample (deflated 27%)\r\nupdating: sklearn_transforms/.git/hooks/pre-rebase.sample (deflated 59%)\r\nupdating: sklearn_transforms/.git/hooks/pre-applypatch.sample (deflated 36%)\r\nupdating: sklearn_transforms/.git/hooks/update.sample (deflated 68%)\r\nupdating: sklearn_transforms/.git/hooks/applypatch-msg.sample (deflated 41%)\r\nupdating: sklearn_transforms/.git/hooks/prepare-commit-msg.sample (deflated 46%)\r\nupdating: sklearn_transforms/.git/hooks/commit-msg.sample (deflated 44%)\r\nupdating: sklearn_transforms/.git/info/ (stored 0%)\r\nupdating: sklearn_transforms/.git/info/exclude (deflated 28%)\r\nupdating: sklearn_transforms/.git/branches/ (stored 0%)\r\nupdating: sklearn_transforms/.git/index (deflated 54%)\r\nupdating: sklearn_transforms/.git/config (deflated 33%)\r\nupdating: sklearn_transforms/.git/description (deflated 14%)\r\nupdating: sklearn_transforms/.git/objects/ (stored 0%)\r\nupdating: sklearn_transforms/.git/objects/info/ (stored 0%)\r\nupdating: sklearn_transforms/.git/objects/pack/ (stored 0%)\r\nupdating: sklearn_transforms/.git/objects/pack/pack-722cb96e26195a9192e8a44b419f50ad0e8f279b.pack (deflated 3%)\r\nupdating: sklearn_transforms/.git/objects/pack/pack-722cb96e26195a9192e8a44b419f50ad0e8f279b.idx (deflated 20%)\r\nupdating: sklearn_transforms/PKG-INFO (deflated 31%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms.egg-info/ (stored 0%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms.egg-info/PKG-INFO (deflated 33%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms.egg-info/not-zip-safe (stored 0%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms.egg-info/SOURCES.txt (deflated 48%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms.egg-info/top_level.txt (stored 0%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms.egg-info/dependency_links.txt (stored 0%)\r\nupdating: sklearn_transforms/setup.cfg (deflated 16%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms/ (stored 0%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms/sklearn_transformers.py (deflated 70%)\r\nupdating: sklearn_transforms/my_custom_sklearn_transforms/__init__.py (stored 0%)\r\nupdating: sklearn_transforms/setup.py (deflated 46%)\r\nupdating: sklearn_transforms/README.md (deflated 15%)\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Con el archivo zip de nuestro paquete cargado en el Kernel de este notebook, podemos usar la herramienta pip para instalarlo, de acuerdo con la celda a continuaci\u00f3n:"}, {"metadata": {}, "cell_type": "code", "source": "!pip install sklearn_transforms.zip", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "Processing ./sklearn_transforms.zip\nBuilding wheels for collected packages: my-custom-sklearn-transforms\n  Building wheel for my-custom-sklearn-transforms (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.tmp/pip-ephem-wheel-cache-4thk6h4v/wheels/8f/88/32/f886e7510a37b111e2a1b7e689e04450acda46732970a7ed78\nSuccessfully built my-custom-sklearn-transforms\nInstalling collected packages: my-custom-sklearn-transforms\n  Found existing installation: my-custom-sklearn-transforms 1.0\n    Uninstalling my-custom-sklearn-transforms-1.0:\n      Successfully uninstalled my-custom-sklearn-transforms-1.0\nSuccessfully installed my-custom-sklearn-transforms-1.0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "\u00a1Ahora podemos importar nuestro paquete personalizado a nuestro notebook!\n\nImportaremos la transformaci\u00f3n DropColumns. Si tienes otras transformaciones personalizadas, \u00a1no olvides importarlas!"}, {"metadata": {}, "cell_type": "code", "source": "from my_custom_sklearn_transforms.sklearn_transformers import DropColumns\nfrom my_custom_sklearn_transforms.sklearn_transformers import Scorecolumn", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Declarando un Pipeline\n\nDespu\u00e9s de importar transformaciones personalizadas como un paquete de Python, podemos proceder a la declaraci\u00f3n de nuestro Pipeline.\n\nEl proceso es muy similar al realizado en la primera etapa, pero con algunas diferencias importantes, \u00a1as\u00ed que presta mucha atenci\u00f3n!\n\nEl Pipeline de ejemplo tiene tres etapas: \n\n    - Remover las colunas \"NAME\" e \"Unnamed: 0\"\n    - Asignar \"ceros\" a todos los valores faltantes\n    - insertar datos preprocesados como entrada en un modelo entrenado\n    \nRecordando, la entrada de este Pipeline ser\u00e1 el conjunto de datos brutos proporcionados, excepto la columna \"PROFILE\" (variable de objetivo que ser\u00e1 determinada por el modelo).\n\nEntonces tendremos 16 valores de entrada en el **PIPELINE** (en el modelo habr\u00e1 14 entradas, ya que las columnas \"NAME\" y \"Unnamed: 0\" se eliminar\u00e1n en la primera etapa despu\u00e9s de la transformaci\u00f3n DropColumns).\n\n\n    Unnamed: 0                          - Esta columna no tiene nombre y debe ser eliminada del dataset\n    NAME                                - Nombre del estudiante\n    USER_ID                             - N\u00famero de identificaci\u00f3n del estudiante\n    HOURS_DATASCIENCE                   - N\u00famero de horas de estudio en Data Science\n    HOURS_BACKEND                       - N\u00famero de horas de estudio en Web (Back-End)\n    HOURS_FRONTEND                      - N\u00famero de horas de estudio en Web (Front-End)\n    NUM_COURSES_BEGINNER_DATASCIENCE    - N\u00famero de cursos de nivel principiante en Data Science completados por el estudiante\n    NUM_COURSES_BEGINNER_BACKEND        - N\u00famero de cursos de nivel principiante en Web (Back-End) completados por el estudiante\n    NUM_COURSES_BEGINNER_FRONTEND       - N\u00famero de cursos de nivel principiante en Web (Front-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_DATASCIENCE    - N\u00famero de cursos de nivel avanzado en Data Science completados por el estudiante\n    NUM_COURSES_ADVANCED_BACKEND        - N\u00famero de cursos de nivel avanzado en Web (Back-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_FRONTEND       - N\u00famero de cursos de nivel avanzado en Web (Front-End) completados por el estudiante\n    AVG_SCORE_DATASCIENCE               - Promedio acumulado en cursos de Data Science completados por el estudiante\n    AVG_SCORE_BACKEND                   - Promedio acumulado en cursos de Web (Back-End) completados por el estudiante\n    AVG_SCORE_FRONTEND                  - Promedio acumulado en cursos de Web (Front-End) completados por el estudiante\n\nLa salida del Pipeline ser\u00e1 un valor estimado para la columna \"PROFILE\"."}, {"metadata": {}, "cell_type": "code", "source": "# Crear una transformaci\u00f3n personalizada ``DropColumns``\n\nrm_columns = DropColumns(\n    columns=[\"NAME\", \"Unnamed: 0\",\"USER_ID\"]\n)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Crear un objeto ``SimpleImputer``\n\nsi = SimpleImputer(\n    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas est\u00e1ndar)\n    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante\n    fill_value=0,  # la constante que se usar\u00e1 para completar los valores faltantes es un int64 = 0\n    verbose=0,\n    copy=True\n)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc=Scorecolumn()", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Definicion de columnas que seran features\nfeatures = [\n    \"Unnamed: 0\", \"NAME\", \"USER_ID\", \"HOURS_DATASCIENCE\", \"HOURS_BACKEND\", \"HOURS_FRONTEND\",\n    \"NUM_COURSES_BEGINNER_DATASCIENCE\", \"NUM_COURSES_BEGINNER_BACKEND\", \"NUM_COURSES_BEGINNER_FRONTEND\",\n    \"NUM_COURSES_ADVANCED_DATASCIENCE\", \"NUM_COURSES_ADVANCED_BACKEND\", \"NUM_COURSES_ADVANCED_FRONTEND\",\n    \"AVG_SCORE_DATASCIENCE\", \"AVG_SCORE_BACKEND\", \"AVG_SCORE_FRONTEND\"\n]\n\n# Definici\u00f3n de variable objetico\ntarget = [\"PROFILE\"]\n\n# Preparaci\u00f3n de argumentos para los m\u00e9todos de la biblioteca ``scikit-learn``\nX = df_data_1[features]\ny = df_data_1[target]\nX.columns", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "Index(['Unnamed: 0', 'NAME', 'USER_ID', 'HOURS_DATASCIENCE', 'HOURS_BACKEND',\n       'HOURS_FRONTEND', 'NUM_COURSES_BEGINNER_DATASCIENCE',\n       'NUM_COURSES_BEGINNER_BACKEND', 'NUM_COURSES_BEGINNER_FRONTEND',\n       'NUM_COURSES_ADVANCED_DATASCIENCE', 'NUM_COURSES_ADVANCED_BACKEND',\n       'NUM_COURSES_ADVANCED_FRONTEND', 'AVG_SCORE_DATASCIENCE',\n       'AVG_SCORE_BACKEND', 'AVG_SCORE_FRONTEND'],\n      dtype='object')"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "**\u00a1\u00a1ATENCI\u00d3N!!**\n\nLa celda de arriba, aunque muy similar a la definici\u00f3n de caracter\u00edsticas en la primera etapa de este desaf\u00edo, \u00a1tiene una gran diferencia!\n\nContiene las columnas \"NAME\" y \"Unnamed: 0\" como features. Esto se debe a que en este caso estas son las entradas *PIPELINE*, no el modelo."}, {"metadata": {}, "cell_type": "code", "source": "# Separaci\u00f3n de datos en un conjunto de entrenamiento y un conjunto de prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "En la celda a continuaci\u00f3n, se declara un objeto scikit-learn **Pipeline**, donde se declara el par\u00e1metro *steps*, que no es m\u00e1s que una lista de los pasos en nuestro pipeline:\n\n    'remove_cols'     - transformaci\u00f3n personalizada DropColumns\n    'imputer'         - transformaci\u00f3n scikit-learn incorporada para asignar valores faltantes\n    'dtc'             - un clasificador a trav\u00e9s del \u00e1rbol de decisi\u00f3n\n\nTenga en cuenta que pasamos como transformaciones instanciadas anteriormente, bajo el nombre `rm_columns` y` si`."}, {"metadata": {}, "cell_type": "code", "source": "# Creaci\u00f3n de nuestro pipeline para almacenamiento en Watson Machine Learning:\nmy_pipeline = Pipeline(\n    steps=[\n        ('remove_cols', rm_columns),\n        ('score',sc),\n        ('imputer', si),\n        ('scaler',MinMaxScaler()),\n        ('knn', KNeighborsClassifier(n_neighbors=4,weights='distance'))\n    ]\n)", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Luego, ejecutaremos el m\u00e9todo `fit ()` de Pipeline, realizando el preprocesamiento y entrenamiento del modelo a la vez."}, {"metadata": {}, "cell_type": "code", "source": "# Inicializaci\u00f3n de Pipeline (preprocesamiento y formaci\u00f3n de modelos)\nmy_pipeline.fit(X_train, y_train)", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/pipeline.py:267: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params)\n", "name": "stderr"}, {"output_type": "execute_result", "execution_count": 15, "data": {"text/plain": "Pipeline(memory=None,\n     steps=[('remove_cols', DropColumns(columns=['NAME', 'Unnamed: 0', 'USER_ID'])), ('score', Scorecolumn(columns=None)), ('imputer', SimpleImputer(copy=True, fill_value=0, missing_values=nan,\n       strategy='constant', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n           weights='distance'))])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y_pred = my_pipeline.predict(X_test)\nfrom sklearn.metrics import accuracy_score\n\n# Precisi\u00f3n lograda por el \u00e1rbol de decisiones\nprint(\"Exactitud: {}%\".format(100*round(accuracy_score(y_test, y_pred), 2)))", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "Exactitud: 93.0%\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Ahora que tenemos un pipeline completo, con los pasos de preprocesamiento configurados y tambi\u00e9n un modelo por \u00e1rbol de decisiones ya entrenado, \u00a1podemos integrarnos con Watson Machine Learning!"}, {"metadata": {}, "cell_type": "markdown", "source": "### Encapsulaci\u00f3n de un Pipeline personalizado en Watson Machine Learning "}, {"metadata": {}, "cell_type": "markdown", "source": "#### Establecer una conexi\u00f3n entre el cliente WML Python y su instancia de servicio en la nube"}, {"metadata": {}, "cell_type": "code", "source": "# Biblioteca de Python con implementaci\u00f3n de un cliente HTTP para la API de WML\nfrom watson_machine_learning_client import WatsonMachineLearningAPIClient", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Las siguientes celdas desplegar\u00e1n el pipeline declarado en este notebook en WML. Contin\u00fae solo si ya est\u00e1 satisfecho con su modelo y cree que es hora de implementar su soluci\u00f3n.\n\nPegue las credenciales de su instancia de Watson Machine Learning en la variable de la celda siguiente.\n\nEs importante que la variable que contiene los valores tenga el nombre de `` wml_credentials`` para que las siguientes celdas de este notebook se ejecuten correctamente."}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n  \"apikey\": \"zmZVon5qzXHeeuPW4gQLCyQn7SMp614iaHzREvu5p0b0\",\n  \"iam_apikey_description\": \"Auto-generated for key 5fd029b9-857d-4180-a5ed-d19621c44a97\",\n  \"iam_apikey_name\": \"Credenciales de servicio-1\",\n  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/7e64e4a5bb0744489faac49874765319::serviceid:ServiceId-6b87fbd0-c2ac-4997-b7cd-e4d6c659e2fa\",\n  \"instance_id\": \"ca50073f-0480-477f-9142-9e3ff6ca96f7\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\"\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Creaci\u00f3n de un objeto cliente de Watson Machine Learning a partir de las credenciales proporcionadas\n\nclientWML = WatsonMachineLearningAPIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Extracci\u00f3n de detalles de su instancia de Watson Machine Learning\n\ninstance_details = clientWML.service_instance.get_details()\nprint(json.dumps(instance_details, indent=4))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**\u00a1\u00a1ATENCI\u00d3N!!**\n\n\u00a1Preste atenci\u00f3n a los l\u00edmites de consumo de su instancia de Watson Machine Learning!\n\nSi expira la capa libre, no podr\u00e1 evaluar su modelo (\u00a1ya que es necesario realizar algunas llamadas a API que consumen predicciones!)"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Listado de todos los artefactos almacenados en su WML"}, {"metadata": {}, "cell_type": "markdown", "source": "Para listar todos los artefactos almacenados en Watson Machine Learning, puede utilizar la siguiente funci\u00f3n:\n\n    clientWML.repository.list()"}, {"metadata": {}, "cell_type": "code", "source": "# Listado de todos los artefactos almacenados en su WML\n\nclientWML.repository.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "En el plan LITE de Watson Machine Learning, solo se puede implementar un modelo a la vez. Si ya tiene un modelo en l\u00ednea en su instancia, puede eliminarlo usando el m\u00e9todo clientWML.repository.delete ():\n\n    artifact_guid = \"359c8951-d2fe-4063-8706-cc06b32d5e0d\"\n    clientWML.repository.delete(artifact_guid)"}, {"metadata": {}, "cell_type": "code", "source": "artifact_guid = \"359c8951-d2fe-4063-8706-cc06b32d5e0d\"\nclientWML.repository.delete(artifact_guid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Crear una nueva definici\u00f3n de paquete Python personalizada en WML"}, {"metadata": {}, "cell_type": "markdown", "source": "El primer paso para realizar su implementaci\u00f3n es almacenar el c\u00f3digo de las transformaciones personalizadas creadas.\n\nPara este paso solo necesitamos el archivo .zip del paquete creado (\u00a1que ya hemos cargado en el Kernel!)"}, {"metadata": {}, "cell_type": "code", "source": "# Definici\u00f3n de metadatos de nuestro paquete con transformaciones personalizadas \npkg_meta = {\n    clientWML.runtimes.LibraryMetaNames.NAME: \"my_custom_sklearn_transform_es_2\",\n    clientWML.runtimes.LibraryMetaNames.DESCRIPTION: \"A custom sklearn transform\",\n    clientWML.runtimes.LibraryMetaNames.FILEPATH: \"sklearn_transforms.zip\",  # Note que estamos utilizando o .zip creado anteriormente!\n    clientWML.runtimes.LibraryMetaNames.VERSION: \"1.0\",\n    clientWML.runtimes.LibraryMetaNames.PLATFORM: { \"name\": \"python\", \"versions\": [\"3.6\"] }\n}\ncustom_package_details = clientWML.runtimes.store_library( pkg_meta )\ncustom_package_uid = clientWML.runtimes.get_library_uid( custom_package_details )\n\nprint(\"\\n Lista de artefactos en tiempo de ejecuci\u00f3n almacenados en WML:\")\nclientWML.repository.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Creaci\u00f3n de una nueva definici\u00f3n personalizada de runtime Python en WML\n\nEl segundo paso es almacenar una definici\u00f3n de runtime Python para usar nuestra biblioteca personalizada.\n\nEsto puede hacerse de la siguiente manera:"}, {"metadata": {}, "cell_type": "code", "source": "runtime_meta = {\n    clientWML.runtimes.ConfigurationMetaNames.NAME: \"my_custom_wml_runtime_es_2\",\n    clientWML.runtimes.ConfigurationMetaNames.DESCRIPTION: \"A Python runtime with custom sklearn Transforms\",\n    clientWML.runtimes.ConfigurationMetaNames.PLATFORM: {\n        \"name\": \"python\",\n        \"version\": \"3.6\"\n    },\n    clientWML.runtimes.ConfigurationMetaNames.LIBRARIES_UIDS: [ custom_package_uid ]\n}\nruntime_details = clientWML.runtimes.store( runtime_meta )\ncustom_runtime_uid = clientWML.runtimes.get_uid( runtime_details )\n\nprint(\"\\n Detalles de runtime almacenados:\")\nprint(json.dumps(runtime_details, indent=4))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Listando todos runtimes armazenados no seu WML:\nclientWML.runtimes.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Crear una nueva definici\u00f3n de Pipeline personalizado en WML\n\nFinalmente crearemos una definici\u00f3n (metadatos) para que nuestro Pipeline se aloje en WML.\n\nDefinimos como par\u00e1metros un nombre para el artefacto y el ID de runtime creado anteriormente."}, {"metadata": {}, "cell_type": "code", "source": "model_meta = {\n    clientWML.repository.ModelMetaNames.NAME: 'desafio-2-mbtc2020-pipeline-es-2',\n    clientWML.repository.ModelMetaNames.DESCRIPTION: \"my pipeline for submission\",\n    clientWML.repository.ModelMetaNames.RUNTIME_UID: custom_runtime_uid\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Luego llamamos al m\u00e9todo para almacenar la nueva definici\u00f3n:"}, {"metadata": {}, "cell_type": "code", "source": "# Funci\u00f3n para almacenar una definici\u00f3n de Pipeline en WML \nstored_model_details = clientWML.repository.store_model(\n    model=my_pipeline,  # `my_pipeline` es la variable creada previamente y contiene nuestro Pipeline ya entrenado :)\n    meta_props=model_meta,  # Metadatos definidos en la celda anterior\n    training_data=None  # No cambie este par\u00e1metro\n)\n\nprint(\"\\n Lista de artefactos almacenados en WML:\")\nclientWML.repository.list()\n\n# Detalles del modelo alojado en Watson Machine Learning\nprint(\"\\n Metadatos almacenados del modelo:\")\nprint(json.dumps(stored_model_details, indent=4))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Despliegue de su modelo para consumo inmediato por otras aplicaciones"}, {"metadata": {}, "cell_type": "code", "source": "# El modelo finalmente se implementa usando el m\u00e9todo `` deployments.create () ``\n\nmodel_deployment_details = clientWML.deployments.create(\n    artifact_uid=stored_model_details[\"metadata\"][\"guid\"],  # No cambie este par\u00e1metro\n    name=\"desafio-2-mbtc2020-deployment-es-2\",\n    description=\"Solu\u00e7\u00e3o do desafio 2 - MBTC\",\n    asynchronous=False,  # No cambie este par\u00e1metro\n    deployment_type='online',  # No cambie este par\u00e1metro\n    deployment_format='Core ML',  # No cambie este par\u00e1metro\n    meta_props=model_meta  # No cambie este par\u00e1metro\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Prueba de un modelo alojado en Watson Machine Learning"}, {"metadata": {}, "cell_type": "code", "source": "# Recuperando a URL endpoint do modelo hospedado na c\u00e9lula anterior\n\nmodel_endpoint_url = clientWML.deployments.get_scoring_url(model_deployment_details)\nprint(\"Su URL de llamada a la API es: {}\".format(model_endpoint_url))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Detalles del despliegue realizado\n\ndeployment_details = clientWML.deployments.get_details(\n    deployment_uid=model_deployment_details[\"metadata\"][\"guid\"]  # este es su ID de implementaci\u00f3n!\n)\n\nprint(\"Metadatos de despliegue realizado: \\n\")\nprint(json.dumps(deployment_details, indent=4))", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "scoring_payload = {\n    'fields': [\n        \"Unnamed: 0\", \"NAME\", \"USER_ID\", \"HOURS_DATASCIENCE\", \"HOURS_BACKEND\", \"HOURS_FRONTEND\",\n        \"NUM_COURSES_BEGINNER_DATASCIENCE\", \"NUM_COURSES_BEGINNER_BACKEND\", \"NUM_COURSES_BEGINNER_FRONTEND\",\n        \"NUM_COURSES_ADVANCED_DATASCIENCE\", \"NUM_COURSES_ADVANCED_BACKEND\", \"NUM_COURSES_ADVANCED_FRONTEND\",\n        \"AVG_SCORE_DATASCIENCE\", \"AVG_SCORE_BACKEND\", \"AVG_SCORE_FRONTEND\"\n    ],\n    'values': [\n        [\n            None,\"Paula Waters\",None,None,None,None,None,None,None,None,None,None,None,None,None,\n        ]\n    ]\n}\n\n# Definici\u00f3n de la variable objetivo\ntarget = [\"PROFILE\"]\n\nprint(\"\\n Payload de datos para clasificar:\")\nprint(json.dumps(scoring_payload, indent=4))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "result = clientWML.deployments.score(\n    model_endpoint_url,\n    scoring_payload\n)\n\nprint(\"\\n Resultados:\")\nprint(json.dumps(result, indent=4))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\n## \u00a1Felicidades! \n\nSi todo sali\u00f3 bien, \u00a1ya tiene un clasificador basado en aprendizaje autom\u00e1tico encapsulado como una API REST!\n\nPara probar tu soluci\u00f3n integrada con un asistente virtual y realizar el env\u00edo, visita la p\u00e1gina:\n\nhttps://tortuga.maratona.dev\n\nNecesitar\u00e1s la URL del endpoint de tu modelo y las credenciales WML :)"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}